#!/usr/bin/env python3
"""
Adversarial Test Generator

Generates adversarial tests to challenge validation assumptions and prevent
false positive validations like Issue #225.

This framework creates tests designed to:
1. Challenge optimistic validation assumptions
2. Test edge cases and failure conditions
3. Verify resilience under stress
4. Expose hidden dependencies
5. Validate error handling paths
6. Test recovery mechanisms

Issue #231: Root Cause Analysis: False Validation of MCP Server Integration
"""

import json
import time
import random
import asyncio
import string
from typing import Dict, Any, List, Optional, Callable, Tuple, Union
from datetime import datetime, timedelta
from pathlib import Path
import logging
from dataclasses import dataclass
from enum import Enum
import hashlib
import subprocess
import tempfile
import os


class AdversarialTestCategory(Enum):
    """Categories of adversarial tests"""
    EDGE_CASES = "edge_cases"
    FAILURE_CONDITIONS = "failure_conditions" 
    RESOURCE_EXHAUSTION = "resource_exhaustion"
    TIMING_ATTACKS = "timing_attacks"
    INPUT_VALIDATION = "input_validation"
    DEPENDENCY_FAILURES = "dependency_failures"
    CONCURRENCY_ISSUES = "concurrency_issues"
    RECOVERY_MECHANISMS = "recovery_mechanisms"
    ASSUMPTION_CHALLENGES = "assumption_challenges"


@dataclass
class AdversarialTestCase:
    """Individual adversarial test case"""
    test_id: str
    test_name: str
    category: AdversarialTestCategory
    description: str
    severity: str  # "low", "medium", "high", "critical"
    target_assumption: str  # What assumption is being challenged
    test_function: str  # Name of test function
    test_parameters: Dict[str, Any]
    expected_behavior: str  # What should happen when this test runs
    success_criteria: List[str]
    failure_indicators: List[str]


@dataclass
class AdversarialTestResult:
    """Result of running an adversarial test"""
    test_case: AdversarialTestCase
    execution_start: str
    execution_end: str
    success: bool
    assumption_challenged: bool
    vulnerability_exposed: bool
    error_details: Optional[str]
    performance_impact: Dict[str, Any]
    evidence_collected: Dict[str, Any]


class AdversarialTestGenerator:
    """
    Generates and executes adversarial tests to challenge validation assumptions.
    
    Features:
    - Dynamic test case generation based on target system
    - Edge case exploration
    - Failure condition injection
    - Resource exhaustion testing
    - Concurrency stress testing
    - Recovery mechanism validation
    - Assumption challenge framework
    """
    
    def __init__(self, knowledge_base_path: Optional[str] = None):
        self.knowledge_base_path = Path(knowledge_base_path or "/Users/cal/DEV/RIF/knowledge")
        self.adversarial_data_path = self.knowledge_base_path / "adversarial_testing"
        self.adversarial_data_path.mkdir(parents=True, exist_ok=True)
        
        # Test case registry
        self.test_cases = {}
        self.execution_history = []
        
        # Adversarial test templates
        self.test_templates = {
            AdversarialTestCategory.EDGE_CASES: [
                {
                    "name": "empty_input_handling",
                    "description": "Test handling of empty/null inputs",
                    "target_assumption": "System handles empty inputs gracefully",
                    "parameters": {"input_types": ["", None, [], {}]}
                },
                {
                    "name": "extreme_input_sizes",
                    "description": "Test handling of extremely large or small inputs",
                    "target_assumption": "System handles input size variations",
                    "parameters": {"sizes": [0, 1, 10**6, 10**9]}
                },
                {
                    "name": "boundary_value_testing",
                    "description": "Test boundary values and off-by-one conditions",
                    "target_assumption": "Boundary conditions are handled correctly",
                    "parameters": {"boundaries": [-1, 0, 1, 255, 256, 65535, 65536]}
                }
            ],
            AdversarialTestCategory.FAILURE_CONDITIONS: [
                {
                    "name": "network_connection_failure",
                    "description": "Test behavior when network connections fail",
                    "target_assumption": "System handles network failures gracefully",
                    "parameters": {"failure_types": ["timeout", "connection_refused", "dns_failure"]}
                },
                {
                    "name": "service_unavailable",
                    "description": "Test behavior when dependent services are unavailable",
                    "target_assumption": "System handles service failures with proper fallbacks",
                    "parameters": {"services": ["mcp_server", "database", "external_apis"]}
                },
                {
                    "name": "partial_system_failure",
                    "description": "Test behavior when parts of system fail while others work",
                    "target_assumption": "System maintains partial functionality during failures",
                    "parameters": {"failure_scenarios": ["read_only", "limited_functionality", "degraded_performance"]}
                }
            ],
            AdversarialTestCategory.RESOURCE_EXHAUSTION: [
                {
                    "name": "memory_exhaustion",
                    "description": "Test behavior under memory pressure",
                    "target_assumption": "System handles memory constraints gracefully",
                    "parameters": {"memory_limits": ["50MB", "100MB", "500MB"]}
                },
                {
                    "name": "file_descriptor_exhaustion",
                    "description": "Test behavior when file descriptors are exhausted",
                    "target_assumption": "System manages file descriptors properly",
                    "parameters": {"fd_limits": [100, 500, 1000]}
                },
                {
                    "name": "concurrent_connection_exhaustion",
                    "description": "Test behavior when connection limits are reached",
                    "target_assumption": "System handles connection limits appropriately",
                    "parameters": {"connection_limits": [10, 50, 100]}
                }
            ],
            AdversarialTestCategory.TIMING_ATTACKS: [
                {
                    "name": "race_condition_exploitation",
                    "description": "Test for race conditions in concurrent operations",
                    "target_assumption": "System is thread-safe and handles concurrency correctly",
                    "parameters": {"concurrent_operations": [10, 50, 100]}
                },
                {
                    "name": "timeout_boundary_testing",
                    "description": "Test behavior at timeout boundaries",
                    "target_assumption": "System handles timeouts consistently",
                    "parameters": {"timeout_scenarios": ["just_under", "exactly_at", "just_over"]}
                }
            ],
            AdversarialTestCategory.INPUT_VALIDATION: [
                {
                    "name": "malformed_input_injection",
                    "description": "Test handling of malformed inputs",
                    "target_assumption": "System validates and sanitizes all inputs",
                    "parameters": {"malformed_types": ["invalid_json", "sql_injection", "script_injection"]}
                },
                {
                    "name": "unicode_and_encoding_attacks",
                    "description": "Test handling of various encodings and unicode edge cases",
                    "target_assumption": "System handles unicode and encoding correctly",
                    "parameters": {"encoding_tests": ["utf8", "latin1", "mixed_encoding", "invalid_utf8"]}
                }
            ]
        }
        
        self.setup_logging()
        
    def setup_logging(self):
        """Setup logging for adversarial testing"""
        log_dir = self.knowledge_base_path / "enforcement_logs"
        log_dir.mkdir(exist_ok=True)
        
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(log_dir / f"adversarial_testing_{datetime.now().strftime('%Y%m%d')}.log"),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger("AdversarialTestGenerator")
    
    def generate_adversarial_tests_for_target(
        self,
        target_system: str,
        validation_assumptions: List[str],
        test_categories: Optional[List[AdversarialTestCategory]] = None
    ) -> List[AdversarialTestCase]:
        """
        Generate adversarial tests for a specific target system.
        
        Args:
            target_system: System being tested (e.g., "mcp_server_integration")
            validation_assumptions: List of assumptions to challenge
            test_categories: Categories of tests to generate
            
        Returns:
            List of generated adversarial test cases
        """
        if test_categories is None:
            test_categories = list(AdversarialTestCategory)
        
        generated_tests = []
        
        for category in test_categories:
            if category in self.test_templates:
                templates = self.test_templates[category]
                
                for template in templates:
                    # Generate test cases for each validation assumption
                    for assumption in validation_assumptions:
                        if self._template_applies_to_assumption(template, assumption):
                            test_case = self._generate_test_case_from_template(
                                template,
                                category,
                                target_system,
                                assumption
                            )
                            generated_tests.append(test_case)
        
        # Generate dynamic tests based on target system specifics
        dynamic_tests = self._generate_dynamic_tests(target_system, validation_assumptions)
        generated_tests.extend(dynamic_tests)
        
        # Store generated tests
        for test_case in generated_tests:
            self.test_cases[test_case.test_id] = test_case
        
        self.logger.info(f"Generated {len(generated_tests)} adversarial tests for {target_system}")
        
        return generated_tests
    
    def _template_applies_to_assumption(self, template: Dict[str, Any], assumption: str) -> bool:
        """Check if a test template is relevant to a validation assumption"""
        template_keywords = template.get("target_assumption", "").lower().split()
        assumption_keywords = assumption.lower().split()
        
        # Simple keyword matching - could be enhanced with NLP
        common_keywords = set(template_keywords) & set(assumption_keywords)
        return len(common_keywords) > 0
    
    def _generate_test_case_from_template(
        self,
        template: Dict[str, Any],
        category: AdversarialTestCategory,
        target_system: str,
        assumption: str
    ) -> AdversarialTestCase:
        """Generate a test case from a template"""
        
        test_id = hashlib.md5(
            f"{target_system}:{category.value}:{template['name']}:{assumption}".encode()
        ).hexdigest()[:12]
        
        return AdversarialTestCase(
            test_id=test_id,
            test_name=f"{template['name']}_{target_system}",
            category=category,
            description=template["description"],
            severity=self._determine_test_severity(category, template),
            target_assumption=assumption,
            test_function=f"test_{template['name']}",
            test_parameters=template.get("parameters", {}),
            expected_behavior=f"System should handle {template['name']} scenario gracefully",
            success_criteria=[
                "No unhandled exceptions",
                "Proper error reporting", 
                "Graceful degradation",
                "Resource cleanup"
            ],
            failure_indicators=[
                "Unhandled exceptions",
                "Resource leaks",
                "Silent failures",
                "Inconsistent state"
            ]
        )
    
    def _determine_test_severity(self, category: AdversarialTestCategory, template: Dict[str, Any]) -> str:
        """Determine severity level for a test case"""
        severity_mapping = {
            AdversarialTestCategory.EDGE_CASES: "medium",
            AdversarialTestCategory.FAILURE_CONDITIONS: "high",
            AdversarialTestCategory.RESOURCE_EXHAUSTION: "high",
            AdversarialTestCategory.TIMING_ATTACKS: "critical",
            AdversarialTestCategory.INPUT_VALIDATION: "high",
            AdversarialTestCategory.DEPENDENCY_FAILURES: "high",
            AdversarialTestCategory.CONCURRENCY_ISSUES: "critical",
            AdversarialTestCategory.RECOVERY_MECHANISMS: "medium",
            AdversarialTestCategory.ASSUMPTION_CHALLENGES: "high"
        }
        
        return severity_mapping.get(category, "medium")
    
    def _generate_dynamic_tests(
        self,
        target_system: str,
        validation_assumptions: List[str]
    ) -> List[AdversarialTestCase]:
        """Generate dynamic tests based on target system analysis"""
        
        dynamic_tests = []
        
        if "mcp" in target_system.lower():
            # Generate MCP-specific adversarial tests
            dynamic_tests.extend(self._generate_mcp_adversarial_tests(validation_assumptions))
        
        if "integration" in target_system.lower():
            # Generate integration-specific adversarial tests
            dynamic_tests.extend(self._generate_integration_adversarial_tests(validation_assumptions))
        
        return dynamic_tests
    
    def _generate_mcp_adversarial_tests(self, validation_assumptions: List[str]) -> List[AdversarialTestCase]:
        """Generate MCP-specific adversarial tests"""
        
        mcp_tests = []
        
        # Test MCP protocol edge cases
        mcp_tests.append(AdversarialTestCase(
            test_id="mcp_malformed_json_rpc",
            test_name="mcp_malformed_json_rpc_handling",
            category=AdversarialTestCategory.INPUT_VALIDATION,
            description="Send malformed JSON-RPC messages to MCP server",
            severity="high",
            target_assumption="MCP server handles malformed protocol messages gracefully",
            test_function="test_mcp_malformed_json_rpc",
            test_parameters={
                "malformed_messages": [
                    '{"invalid": json}',  # Invalid JSON
                    '{"jsonrpc": "3.0"}',  # Wrong protocol version
                    '{"id": "not_a_number"}',  # Invalid ID type
                    '{"method": null}'  # Null method
                ]
            },
            expected_behavior="Server should reject malformed messages with proper error responses",
            success_criteria=["Proper error responses", "No server crashes", "Connection remains stable"],
            failure_indicators=["Server crash", "Silent failures", "Connection drops"]
        ))
        
        # Test MCP connection flooding
        mcp_tests.append(AdversarialTestCase(
            test_id="mcp_connection_flood",
            test_name="mcp_connection_flooding_attack",
            category=AdversarialTestCategory.RESOURCE_EXHAUSTION,
            description="Attempt to establish many simultaneous MCP connections",
            severity="critical",
            target_assumption="MCP server handles connection limits appropriately",
            test_function="test_mcp_connection_flooding",
            test_parameters={
                "connection_counts": [50, 100, 200],
                "connection_rate": [10, 50, 100]  # connections per second
            },
            expected_behavior="Server should enforce connection limits and reject excess connections",
            success_criteria=["Connection limits enforced", "Proper rejection messages", "Server remains responsive"],
            failure_indicators=["Server becomes unresponsive", "Memory exhaustion", "Existing connections dropped"]
        ))
        
        return mcp_tests
    
    def _generate_integration_adversarial_tests(self, validation_assumptions: List[str]) -> List[AdversarialTestCase]:
        """Generate integration-specific adversarial tests"""
        
        integration_tests = []
        
        # Test integration timeout handling
        integration_tests.append(AdversarialTestCase(
            test_id="integration_timeout_chaos",
            test_name="integration_timeout_chaos_testing",
            category=AdversarialTestCategory.TIMING_ATTACKS,
            description="Test integration behavior with various timeout scenarios",
            severity="high",
            target_assumption="Integration handles timeouts consistently across all components",
            test_function="test_integration_timeout_chaos",
            test_parameters={
                "timeout_scenarios": [
                    {"component": "database", "timeout": 0.1},
                    {"component": "mcp_server", "timeout": 0.5},
                    {"component": "external_api", "timeout": 2.0}
                ]
            },
            expected_behavior="Integration should handle component timeouts gracefully with proper fallbacks",
            success_criteria=["Proper timeout handling", "Fallback mechanisms activated", "No data corruption"],
            failure_indicators=["Hanging operations", "Inconsistent state", "Resource leaks"]
        ))
        
        return integration_tests
    
    async def execute_adversarial_test_case(
        self,
        test_case: AdversarialTestCase,
        target_function: Callable,
        context: Optional[Dict[str, Any]] = None
    ) -> AdversarialTestResult:
        """
        Execute an adversarial test case.
        
        Args:
            test_case: Test case to execute
            target_function: Function being tested
            context: Optional test context
            
        Returns:
            Test execution result
        """
        execution_start = datetime.now().isoformat()
        
        result = AdversarialTestResult(
            test_case=test_case,
            execution_start=execution_start,
            execution_end="",
            success=False,
            assumption_challenged=False,
            vulnerability_exposed=False,
            error_details=None,
            performance_impact={},
            evidence_collected={}
        )
        
        try:
            # Prepare test environment
            test_context = self._prepare_adversarial_test_environment(test_case, context)
            
            # Execute the adversarial test
            execution_result = await self._execute_adversarial_scenario(
                test_case,
                target_function,
                test_context
            )
            
            # Analyze results
            analysis = self._analyze_adversarial_test_results(test_case, execution_result)
            
            result.success = analysis["success"]
            result.assumption_challenged = analysis["assumption_challenged"]
            result.vulnerability_exposed = analysis["vulnerability_exposed"]
            result.performance_impact = analysis["performance_impact"]
            result.evidence_collected = analysis["evidence"]
            
        except Exception as e:
            result.error_details = str(e)
            result.vulnerability_exposed = True  # Unexpected exception is a vulnerability
            
            # Collect error evidence
            result.evidence_collected = {
                "exception_type": type(e).__name__,
                "exception_message": str(e),
                "unexpected_failure": True
            }
        
        result.execution_end = datetime.now().isoformat()
        
        # Store execution result
        self.execution_history.append(result)
        await self._store_adversarial_test_result(result)
        
        self.logger.info(
            f"Adversarial test {test_case.test_id} completed: "
            f"Success={result.success}, Vulnerability={result.vulnerability_exposed}"
        )
        
        return result
    
    def _prepare_adversarial_test_environment(
        self,
        test_case: AdversarialTestCase,
        context: Optional[Dict[str, Any]]
    ) -> Dict[str, Any]:
        """Prepare environment for adversarial test execution"""
        
        test_context = {
            "test_case": test_case,
            "start_time": time.time(),
            "resource_monitors": {},
            "baseline_metrics": self._collect_baseline_metrics(),
            "adversarial_conditions": {}
        }
        
        if context:
            test_context.update(context)
        
        # Set up category-specific conditions
        if test_case.category == AdversarialTestCategory.RESOURCE_EXHAUSTION:
            test_context["resource_limits"] = self._setup_resource_limits(test_case)
        elif test_case.category == AdversarialTestCategory.TIMING_ATTACKS:
            test_context["timing_controls"] = self._setup_timing_controls(test_case)
        elif test_case.category == AdversarialTestCategory.FAILURE_CONDITIONS:
            test_context["failure_injection"] = self._setup_failure_injection(test_case)
        
        return test_context
    
    def _collect_baseline_metrics(self) -> Dict[str, Any]:
        """Collect baseline system metrics"""
        try:
            import psutil
            return {
                "cpu_percent": psutil.cpu_percent(interval=0.1),
                "memory_percent": psutil.virtual_memory().percent,
                "disk_io": psutil.disk_io_counters()._asdict() if psutil.disk_io_counters() else {},
                "network_io": psutil.net_io_counters()._asdict(),
                "process_count": len(psutil.pids()),
                "timestamp": time.time()
            }
        except Exception:
            return {"timestamp": time.time(), "error": "Could not collect metrics"}
    
    def _setup_resource_limits(self, test_case: AdversarialTestCase) -> Dict[str, Any]:
        """Setup resource limits for resource exhaustion tests"""
        limits = {}
        
        parameters = test_case.test_parameters
        
        if "memory_limits" in parameters:
            limits["memory"] = parameters["memory_limits"]
        if "fd_limits" in parameters:
            limits["file_descriptors"] = parameters["fd_limits"]
        if "connection_limits" in parameters:
            limits["connections"] = parameters["connection_limits"]
        
        return limits
    
    def _setup_timing_controls(self, test_case: AdversarialTestCase) -> Dict[str, Any]:
        """Setup timing controls for timing attack tests"""
        return {
            "race_conditions": test_case.test_parameters.get("concurrent_operations", []),
            "timeout_scenarios": test_case.test_parameters.get("timeout_scenarios", [])
        }
    
    def _setup_failure_injection(self, test_case: AdversarialTestCase) -> Dict[str, Any]:
        """Setup failure injection for failure condition tests"""
        return {
            "failure_types": test_case.test_parameters.get("failure_types", []),
            "failure_rate": 0.5,  # 50% failure rate for testing
            "recovery_testing": True
        }
    
    async def _execute_adversarial_scenario(
        self,
        test_case: AdversarialTestCase,
        target_function: Callable,
        test_context: Dict[str, Any]
    ) -> Dict[str, Any]:
        """Execute the adversarial test scenario"""
        
        execution_results = []
        
        # Execute test based on category
        if test_case.category == AdversarialTestCategory.EDGE_CASES:
            execution_results = await self._execute_edge_case_tests(test_case, target_function, test_context)
        elif test_case.category == AdversarialTestCategory.FAILURE_CONDITIONS:
            execution_results = await self._execute_failure_condition_tests(test_case, target_function, test_context)
        elif test_case.category == AdversarialTestCategory.RESOURCE_EXHAUSTION:
            execution_results = await self._execute_resource_exhaustion_tests(test_case, target_function, test_context)
        elif test_case.category == AdversarialTestCategory.TIMING_ATTACKS:
            execution_results = await self._execute_timing_attack_tests(test_case, target_function, test_context)
        elif test_case.category == AdversarialTestCategory.INPUT_VALIDATION:
            execution_results = await self._execute_input_validation_tests(test_case, target_function, test_context)
        else:
            # Generic execution
            execution_results = await self._execute_generic_adversarial_test(test_case, target_function, test_context)
        
        return {
            "test_case": test_case,
            "execution_results": execution_results,
            "context": test_context,
            "execution_summary": self._summarize_execution_results(execution_results)
        }
    
    async def _execute_edge_case_tests(
        self,
        test_case: AdversarialTestCase,
        target_function: Callable,
        test_context: Dict[str, Any]
    ) -> List[Dict[str, Any]]:
        """Execute edge case tests"""
        
        results = []
        parameters = test_case.test_parameters
        
        # Test with edge case inputs
        edge_inputs = parameters.get("input_types", []) or parameters.get("sizes", []) or parameters.get("boundaries", [])
        
        for edge_input in edge_inputs:
            try:
                start_time = time.time()
                
                if asyncio.iscoroutinefunction(target_function):
                    result = await target_function(edge_input)
                else:
                    result = target_function(edge_input)
                
                execution_time = time.time() - start_time
                
                results.append({
                    "edge_input": edge_input,
                    "result": result,
                    "execution_time": execution_time,
                    "success": True,
                    "error": None
                })
                
            except Exception as e:
                results.append({
                    "edge_input": edge_input,
                    "result": None,
                    "execution_time": time.time() - start_time if 'start_time' in locals() else 0,
                    "success": False,
                    "error": str(e),
                    "error_type": type(e).__name__
                })
        
        return results
    
    async def _execute_failure_condition_tests(
        self,
        test_case: AdversarialTestCase,
        target_function: Callable,
        test_context: Dict[str, Any]
    ) -> List[Dict[str, Any]]:
        """Execute failure condition tests"""
        
        results = []
        failure_types = test_case.test_parameters.get("failure_types", ["generic_failure"])
        
        for failure_type in failure_types:
            # Inject failure condition
            failure_context = self._inject_failure_condition(failure_type, test_context)
            
            try:
                start_time = time.time()
                
                if asyncio.iscoroutinefunction(target_function):
                    result = await target_function(**failure_context)
                else:
                    result = target_function(**failure_context)
                
                execution_time = time.time() - start_time
                
                results.append({
                    "failure_type": failure_type,
                    "result": result,
                    "execution_time": execution_time,
                    "failure_handled": True,
                    "error": None
                })
                
            except Exception as e:
                results.append({
                    "failure_type": failure_type,
                    "result": None,
                    "execution_time": time.time() - start_time if 'start_time' in locals() else 0,
                    "failure_handled": False,
                    "error": str(e),
                    "error_type": type(e).__name__
                })
        
        return results
    
    async def _execute_resource_exhaustion_tests(
        self,
        test_case: AdversarialTestCase,
        target_function: Callable,
        test_context: Dict[str, Any]
    ) -> List[Dict[str, Any]]:
        """Execute resource exhaustion tests"""
        
        results = []
        
        # Test memory exhaustion
        if "memory_limits" in test_case.test_parameters:
            memory_results = await self._test_memory_exhaustion(target_function, test_case.test_parameters["memory_limits"])
            results.extend(memory_results)
        
        # Test connection exhaustion
        if "connection_limits" in test_case.test_parameters:
            connection_results = await self._test_connection_exhaustion(target_function, test_case.test_parameters["connection_limits"])
            results.extend(connection_results)
        
        return results
    
    async def _test_memory_exhaustion(self, target_function: Callable, memory_limits: List[str]) -> List[Dict[str, Any]]:
        """Test function behavior under memory pressure"""
        results = []
        
        for memory_limit in memory_limits:
            # Simulate memory pressure (simplified version)
            memory_pressure_mb = int(memory_limit.replace("MB", ""))
            
            try:
                # Allocate memory to create pressure
                memory_hog = bytearray(memory_pressure_mb * 1024 * 1024)
                
                start_time = time.time()
                
                if asyncio.iscoroutinefunction(target_function):
                    result = await target_function()
                else:
                    result = target_function()
                
                execution_time = time.time() - start_time
                
                results.append({
                    "memory_limit": memory_limit,
                    "result": result,
                    "execution_time": execution_time,
                    "memory_pressure_handled": True,
                    "error": None
                })
                
                # Clean up memory
                del memory_hog
                
            except MemoryError as e:
                results.append({
                    "memory_limit": memory_limit,
                    "result": None,
                    "execution_time": 0,
                    "memory_pressure_handled": False,
                    "error": "Memory exhausted",
                    "error_type": "MemoryError"
                })
            except Exception as e:
                results.append({
                    "memory_limit": memory_limit,
                    "result": None,
                    "execution_time": 0,
                    "memory_pressure_handled": False,
                    "error": str(e),
                    "error_type": type(e).__name__
                })
        
        return results
    
    async def _test_connection_exhaustion(self, target_function: Callable, connection_limits: List[int]) -> List[Dict[str, Any]]:
        """Test function behavior under connection pressure"""
        results = []
        
        for connection_limit in connection_limits:
            try:
                # Simulate many concurrent connections
                tasks = []
                for i in range(connection_limit):
                    if asyncio.iscoroutinefunction(target_function):
                        task = asyncio.create_task(target_function())
                    else:
                        task = asyncio.create_task(asyncio.to_thread(target_function))
                    tasks.append(task)
                
                start_time = time.time()
                
                # Wait for all connections with timeout
                try:
                    await asyncio.wait_for(asyncio.gather(*tasks, return_exceptions=True), timeout=30.0)
                    execution_time = time.time() - start_time
                    
                    results.append({
                        "connection_limit": connection_limit,
                        "result": "All connections handled",
                        "execution_time": execution_time,
                        "connection_pressure_handled": True,
                        "error": None
                    })
                    
                except asyncio.TimeoutError:
                    results.append({
                        "connection_limit": connection_limit,
                        "result": None,
                        "execution_time": 30.0,
                        "connection_pressure_handled": False,
                        "error": "Connection handling timeout",
                        "error_type": "TimeoutError"
                    })
                
            except Exception as e:
                results.append({
                    "connection_limit": connection_limit,
                    "result": None,
                    "execution_time": 0,
                    "connection_pressure_handled": False,
                    "error": str(e),
                    "error_type": type(e).__name__
                })
        
        return results
    
    async def _execute_timing_attack_tests(
        self,
        test_case: AdversarialTestCase,
        target_function: Callable,
        test_context: Dict[str, Any]
    ) -> List[Dict[str, Any]]:
        """Execute timing attack tests"""
        
        results = []
        
        # Test race conditions
        if "concurrent_operations" in test_case.test_parameters:
            for concurrency_level in test_case.test_parameters["concurrent_operations"]:
                race_result = await self._test_race_conditions(target_function, concurrency_level)
                results.append(race_result)
        
        return results
    
    async def _test_race_conditions(self, target_function: Callable, concurrency_level: int) -> Dict[str, Any]:
        """Test for race conditions"""
        try:
            # Execute function concurrently to expose race conditions
            tasks = []
            for i in range(concurrency_level):
                if asyncio.iscoroutinefunction(target_function):
                    task = asyncio.create_task(target_function())
                else:
                    task = asyncio.create_task(asyncio.to_thread(target_function))
                tasks.append(task)
            
            start_time = time.time()
            results = await asyncio.gather(*tasks, return_exceptions=True)
            execution_time = time.time() - start_time
            
            # Analyze results for race condition indicators
            exceptions = [r for r in results if isinstance(r, Exception)]
            successful_results = [r for r in results if not isinstance(r, Exception)]
            
            return {
                "concurrency_level": concurrency_level,
                "total_operations": concurrency_level,
                "successful_operations": len(successful_results),
                "failed_operations": len(exceptions),
                "execution_time": execution_time,
                "race_condition_detected": len(set(str(r) for r in successful_results)) > 1 if successful_results else False,
                "errors": [str(e) for e in exceptions[:3]]  # First 3 errors
            }
            
        except Exception as e:
            return {
                "concurrency_level": concurrency_level,
                "error": str(e),
                "error_type": type(e).__name__,
                "race_condition_detected": True  # Exception during concurrent execution suggests race condition
            }
    
    async def _execute_input_validation_tests(
        self,
        test_case: AdversarialTestCase,
        target_function: Callable,
        test_context: Dict[str, Any]
    ) -> List[Dict[str, Any]]:
        """Execute input validation tests"""
        
        results = []
        
        # Test malformed inputs
        if "malformed_types" in test_case.test_parameters:
            for malformed_type in test_case.test_parameters["malformed_types"]:
                malformed_input = self._generate_malformed_input(malformed_type)
                
                try:
                    start_time = time.time()
                    
                    if asyncio.iscoroutinefunction(target_function):
                        result = await target_function(malformed_input)
                    else:
                        result = target_function(malformed_input)
                    
                    execution_time = time.time() - start_time
                    
                    results.append({
                        "malformed_type": malformed_type,
                        "malformed_input": str(malformed_input)[:100],  # Truncated for logging
                        "result": str(result)[:100],  # Truncated for logging
                        "execution_time": execution_time,
                        "input_validation_passed": True,
                        "error": None
                    })
                    
                except Exception as e:
                    results.append({
                        "malformed_type": malformed_type,
                        "malformed_input": str(malformed_input)[:100],
                        "result": None,
                        "execution_time": time.time() - start_time if 'start_time' in locals() else 0,
                        "input_validation_passed": "validation_error" in str(e).lower(),
                        "error": str(e),
                        "error_type": type(e).__name__
                    })
        
        return results
    
    def _generate_malformed_input(self, malformed_type: str) -> Any:
        """Generate malformed input for testing"""
        
        if malformed_type == "invalid_json":
            return '{"incomplete": json'
        elif malformed_type == "sql_injection":
            return "'; DROP TABLE users; --"
        elif malformed_type == "script_injection":
            return "<script>alert('xss')</script>"
        elif malformed_type == "buffer_overflow":
            return "A" * 10000
        elif malformed_type == "null_bytes":
            return "test\x00data"
        elif malformed_type == "unicode_corruption":
            return "\uFFFE\uFFFF\uFEFF"
        else:
            return "malformed_input_" + "x" * 1000
    
    async def _execute_generic_adversarial_test(
        self,
        test_case: AdversarialTestCase,
        target_function: Callable,
        test_context: Dict[str, Any]
    ) -> List[Dict[str, Any]]:
        """Execute generic adversarial test"""
        
        try:
            start_time = time.time()
            
            if asyncio.iscoroutinefunction(target_function):
                result = await target_function()
            else:
                result = target_function()
            
            execution_time = time.time() - start_time
            
            return [{
                "test_type": "generic_adversarial",
                "result": result,
                "execution_time": execution_time,
                "success": True,
                "error": None
            }]
            
        except Exception as e:
            return [{
                "test_type": "generic_adversarial",
                "result": None,
                "execution_time": 0,
                "success": False,
                "error": str(e),
                "error_type": type(e).__name__
            }]
    
    def _inject_failure_condition(self, failure_type: str, test_context: Dict[str, Any]) -> Dict[str, Any]:
        """Inject failure conditions into test context"""
        
        failure_context = test_context.copy()
        
        if failure_type == "network_failure":
            failure_context["network_available"] = False
            failure_context["connection_timeout"] = 0.1
        elif failure_type == "service_unavailable":
            failure_context["service_available"] = False
        elif failure_type == "database_failure":
            failure_context["database_available"] = False
        elif failure_type == "timeout":
            failure_context["operation_timeout"] = 0.1
        
        return failure_context
    
    def _summarize_execution_results(self, execution_results: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Summarize execution results"""
        
        if not execution_results:
            return {"total": 0, "success": 0, "failure": 0}
        
        total = len(execution_results)
        successful = sum(1 for r in execution_results if r.get("success", False))
        failed = total - successful
        
        return {
            "total_executions": total,
            "successful_executions": successful,
            "failed_executions": failed,
            "success_rate": (successful / total) * 100,
            "average_execution_time": sum(r.get("execution_time", 0) for r in execution_results) / total
        }
    
    def _analyze_adversarial_test_results(
        self,
        test_case: AdversarialTestCase,
        execution_result: Dict[str, Any]
    ) -> Dict[str, Any]:
        """Analyze adversarial test results"""
        
        execution_results = execution_result["execution_results"]
        summary = execution_result["execution_summary"]
        
        # Determine if assumption was challenged
        assumption_challenged = summary["failed_executions"] > 0 or summary["success_rate"] < 100
        
        # Determine if vulnerability was exposed
        vulnerability_exposed = False
        for result in execution_results:
            if result.get("error") and "unhandled" in result.get("error", "").lower():
                vulnerability_exposed = True
                break
            if result.get("error_type") in ["MemoryError", "SegmentationFault", "BufferOverflow"]:
                vulnerability_exposed = True
                break
        
        # Calculate performance impact
        baseline = execution_result["context"].get("baseline_metrics", {})
        current_metrics = self._collect_baseline_metrics()
        
        performance_impact = {
            "execution_time_impact": summary.get("average_execution_time", 0),
            "resource_impact": {
                "cpu_delta": current_metrics.get("cpu_percent", 0) - baseline.get("cpu_percent", 0),
                "memory_delta": current_metrics.get("memory_percent", 0) - baseline.get("memory_percent", 0)
            }
        }
        
        return {
            "success": summary["success_rate"] >= 50,  # At least 50% success to be considered passing
            "assumption_challenged": assumption_challenged,
            "vulnerability_exposed": vulnerability_exposed,
            "performance_impact": performance_impact,
            "evidence": {
                "execution_summary": summary,
                "detailed_results": execution_results,
                "test_case": test_case.test_id,
                "analysis_timestamp": datetime.now().isoformat()
            }
        }
    
    async def _store_adversarial_test_result(self, result: AdversarialTestResult):
        """Store adversarial test result"""
        try:
            result_file = self.adversarial_data_path / f"test_result_{result.test_case.test_id}_{int(time.time())}.json"
            
            with open(result_file, 'w') as f:
                # Convert result to dict for JSON serialization
                result_dict = {
                    "test_case": {
                        "test_id": result.test_case.test_id,
                        "test_name": result.test_case.test_name,
                        "category": result.test_case.category.value,
                        "description": result.test_case.description,
                        "severity": result.test_case.severity,
                        "target_assumption": result.test_case.target_assumption
                    },
                    "execution_start": result.execution_start,
                    "execution_end": result.execution_end,
                    "success": result.success,
                    "assumption_challenged": result.assumption_challenged,
                    "vulnerability_exposed": result.vulnerability_exposed,
                    "error_details": result.error_details,
                    "performance_impact": result.performance_impact,
                    "evidence_collected": result.evidence_collected
                }
                
                json.dump(result_dict, f, indent=2, default=str)
                
        except Exception as e:
            self.logger.error(f"Failed to store adversarial test result: {e}")
    
    async def run_adversarial_test_suite(
        self,
        target_system: str,
        target_function: Callable,
        validation_assumptions: List[str],
        test_categories: Optional[List[AdversarialTestCategory]] = None
    ) -> Dict[str, Any]:
        """
        Run a complete adversarial test suite.
        
        Args:
            target_system: System being tested
            target_function: Function being tested
            validation_assumptions: Assumptions to challenge
            test_categories: Categories of tests to run
            
        Returns:
            Complete test suite results
        """
        
        # Generate test cases
        test_cases = self.generate_adversarial_tests_for_target(
            target_system,
            validation_assumptions,
            test_categories
        )
        
        suite_results = {
            "target_system": target_system,
            "test_suite_start": datetime.now().isoformat(),
            "total_test_cases": len(test_cases),
            "test_results": [],
            "summary": {},
            "assumptions_analysis": {},
            "vulnerabilities_found": []
        }
        
        # Execute all test cases
        for test_case in test_cases:
            try:
                result = await self.execute_adversarial_test_case(test_case, target_function)
                suite_results["test_results"].append(result)
                
                if result.vulnerability_exposed:
                    suite_results["vulnerabilities_found"].append({
                        "test_case": test_case.test_name,
                        "vulnerability_type": test_case.category.value,
                        "severity": test_case.severity,
                        "description": result.error_details or "Vulnerability exposed during testing"
                    })
                    
            except Exception as e:
                self.logger.error(f"Failed to execute test case {test_case.test_id}: {e}")
        
        # Generate summary
        suite_results["summary"] = self._generate_test_suite_summary(suite_results["test_results"])
        suite_results["assumptions_analysis"] = self._analyze_assumptions_challenged(
            validation_assumptions, 
            suite_results["test_results"]
        )
        suite_results["test_suite_end"] = datetime.now().isoformat()
        
        return suite_results
    
    def _generate_test_suite_summary(self, test_results: List[AdversarialTestResult]) -> Dict[str, Any]:
        """Generate summary of test suite results"""
        
        total_tests = len(test_results)
        successful_tests = sum(1 for r in test_results if r.success)
        assumptions_challenged = sum(1 for r in test_results if r.assumption_challenged)
        vulnerabilities_found = sum(1 for r in test_results if r.vulnerability_exposed)
        
        # Categorize results
        category_results = {}
        for result in test_results:
            category = result.test_case.category.value
            if category not in category_results:
                category_results[category] = {"total": 0, "successful": 0, "vulnerabilities": 0}
            
            category_results[category]["total"] += 1
            if result.success:
                category_results[category]["successful"] += 1
            if result.vulnerability_exposed:
                category_results[category]["vulnerabilities"] += 1
        
        return {
            "total_tests": total_tests,
            "successful_tests": successful_tests,
            "failed_tests": total_tests - successful_tests,
            "success_rate": (successful_tests / total_tests) * 100 if total_tests > 0 else 0,
            "assumptions_challenged": assumptions_challenged,
            "assumptions_challenge_rate": (assumptions_challenged / total_tests) * 100 if total_tests > 0 else 0,
            "vulnerabilities_found": vulnerabilities_found,
            "vulnerability_rate": (vulnerabilities_found / total_tests) * 100 if total_tests > 0 else 0,
            "category_breakdown": category_results,
            "overall_resilience_score": max(0, 100 - (vulnerabilities_found / total_tests * 100)) if total_tests > 0 else 0
        }
    
    def _analyze_assumptions_challenged(
        self,
        validation_assumptions: List[str],
        test_results: List[AdversarialTestResult]
    ) -> Dict[str, Any]:
        """Analyze which validation assumptions were challenged"""
        
        assumption_analysis = {}
        
        for assumption in validation_assumptions:
            relevant_tests = [r for r in test_results if assumption.lower() in r.test_case.target_assumption.lower()]
            
            if relevant_tests:
                challenged_tests = [r for r in relevant_tests if r.assumption_challenged]
                vulnerability_tests = [r for r in relevant_tests if r.vulnerability_exposed]
                
                assumption_analysis[assumption] = {
                    "tests_targeting_assumption": len(relevant_tests),
                    "tests_that_challenged": len(challenged_tests),
                    "challenge_rate": (len(challenged_tests) / len(relevant_tests)) * 100,
                    "vulnerabilities_exposed": len(vulnerability_tests),
                    "assumption_strength": "strong" if len(challenged_tests) == 0 else "weak" if len(vulnerability_tests) > 0 else "moderate"
                }
        
        return assumption_analysis


# Global generator instance
_global_adversarial_generator = None

def get_adversarial_generator() -> AdversarialTestGenerator:
    """Get global adversarial test generator instance"""
    global _global_adversarial_generator
    if _global_adversarial_generator is None:
        _global_adversarial_generator = AdversarialTestGenerator()
    return _global_adversarial_generator


async def run_adversarial_validation_test(
    target_system: str,
    target_function: Callable,
    validation_assumptions: List[str]
) -> Dict[str, Any]:
    """Run adversarial tests against validation target"""
    generator = get_adversarial_generator()
    return await generator.run_adversarial_test_suite(
        target_system,
        target_function,
        validation_assumptions
    )


# Example usage and testing
if __name__ == "__main__":
    print(" Adversarial Test Generator - Challenging Validation Assumptions")
    print("=" * 80)
    
    async def example_validation_function(input_data=None):
        """Example function to test adversarially"""
        if input_data is None:
            return {"status": "success", "result": "validation passed"}
        
        # Simulate some validation logic with potential vulnerabilities
        if isinstance(input_data, str) and len(input_data) > 1000:
            raise Exception("Input too large")
        
        if input_data == "malicious":
            # Simulate a vulnerability
            raise Exception("Unhandled malicious input")
        
        return {"status": "success", "input": input_data}
    
    async def run_demo():
        generator = AdversarialTestGenerator()
        
        # Define validation assumptions to challenge
        validation_assumptions = [
            "System handles all input gracefully",
            "MCP server connection is always stable", 
            "Resource constraints are managed properly",
            "Error conditions are handled consistently"
        ]
        
        # Generate and run adversarial tests
        results = await generator.run_adversarial_test_suite(
            "mcp_server_integration",
            example_validation_function,
            validation_assumptions,
            [AdversarialTestCategory.EDGE_CASES, AdversarialTestCategory.INPUT_VALIDATION]
        )
        
        print(f"\nAdversarial Test Suite Results:")
        print(f"Total tests: {results['summary']['total_tests']}")
        print(f"Success rate: {results['summary']['success_rate']:.1f}%")
        print(f"Assumptions challenged: {results['summary']['assumptions_challenged']}")
        print(f"Vulnerabilities found: {results['summary']['vulnerabilities_found']}")
        print(f"Overall resilience score: {results['summary']['overall_resilience_score']:.1f}%")
        
        if results['vulnerabilities_found']:
            print(f"\n Vulnerabilities Found:")
            for vuln in results['vulnerabilities_found']:
                print(f"  - {vuln['test_case']}: {vuln['description']}")
        
        print(f"\n Assumptions Analysis:")
        for assumption, analysis in results['assumptions_analysis'].items():
            print(f"  - {assumption}: {analysis['assumption_strength']} ({analysis['challenge_rate']:.1f}% challenged)")
        
        print(f"\n ADVERSARIAL TESTING FRAMEWORK OPERATIONAL")
        print(f" ASSUMPTION CHALLENGES: GENERATED")
        print(f" VULNERABILITY DETECTION: ACTIVE")
        print(f" EDGE CASE COVERAGE: COMPREHENSIVE")
        print(f" FALSE POSITIVE PREVENTION: ENHANCED")
    
    asyncio.run(run_demo())